{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# NLP_Summarization_Code.ipynb\n",
        "# -------------------------------------------------------------\n",
        "# 1. Install Dependencies (uncomment if running locally)\n",
        "# -------------------------------------------------------------\n",
        "# !pip install transformers rouge-score pandas\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. Import Libraries\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. Load Dataset\n",
        "# -------------------------------------------------------------\n",
        "# Small demo dataset for summarization\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and humans through language. \"\n",
        "        \"It has applications in chatbots, machine translation, sentiment analysis, and summarization.\",\n",
        "        \"Transformers are a powerful model architecture in NLP. \"\n",
        "        \"Models like BERT, GPT, and T5 have revolutionized text understanding and generation tasks.\",\n",
        "        \"Data preprocessing is essential for NLP tasks. Tokenization, lowercasing, and removing special characters help models learn effectively.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Dataset Loaded:\\n\", df)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. Initialize Summarization Pipeline\n",
        "# -------------------------------------------------------------\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. Generate Summaries\n",
        "# -------------------------------------------------------------\n",
        "summaries = []\n",
        "for text in df['text']:\n",
        "    summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n",
        "    summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "df['summary'] = summaries\n",
        "print(\"\\nSummaries Generated:\\n\", df[['text', 'summary']])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6. Evaluate Summaries using ROUGE\n",
        "# -------------------------------------------------------------\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    scores = scorer.score(row['text'], row['summary'])\n",
        "    print(f\"\\nOriginal Text:\\n{row['text']}\")\n",
        "    print(f\"Summary:\\n{row['summary']}\")\n",
        "    print(f\"ROUGE Scores: {scores}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbj6GnCWN-gF",
        "outputId": "c8572e28-6a18-4f4b-dd3b-a7b2d859f49e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "                                                 text\n",
            "0  Natural Language Processing (NLP) is a field o...\n",
            "1  Transformers are a powerful model architecture...\n",
            "2  Data preprocessing is essential for NLP tasks....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 50, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
            "Your max_length is set to 50, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
            "Your max_length is set to 50, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summaries Generated:\n",
            "                                                 text  \\\n",
            "0  Natural Language Processing (NLP) is a field o...   \n",
            "1  Transformers are a powerful model architecture...   \n",
            "2  Data preprocessing is essential for NLP tasks....   \n",
            "\n",
            "                                             summary  \n",
            "0  Natural Language Processing (NLP) is a field o...  \n",
            "1  Transformers are powerful model architecture i...  \n",
            "2  Data preprocessing is essential for NLP tasks....  \n",
            "\n",
            "Original Text:\n",
            "Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and humans through language. It has applications in chatbots, machine translation, sentiment analysis, and summarization.\n",
            "Summary:\n",
            "Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and humans through language. It has applications in chatbots, machine translation, sentiment analysis, and summarization.\n",
            "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
            "\n",
            "Original Text:\n",
            "Transformers are a powerful model architecture in NLP. Models like BERT, GPT, and T5 have revolutionized text understanding and generation tasks.\n",
            "Summary:\n",
            "Transformers are powerful model architecture in NLP. Models like BERT, GPT, and T5 have revolutionized text understanding and generation tasks.\n",
            "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.9523809523809523, fmeasure=0.975609756097561), 'rouge2': Score(precision=0.9473684210526315, recall=0.9, fmeasure=0.9230769230769231), 'rougeL': Score(precision=1.0, recall=0.9523809523809523, fmeasure=0.975609756097561)}\n",
            "\n",
            "Original Text:\n",
            "Data preprocessing is essential for NLP tasks. Tokenization, lowercasing, and removing special characters help models learn effectively.\n",
            "Summary:\n",
            "Data preprocessing is essential for NLP tasks. Tokenization, lowercasing, and removing special characters help models learn effectively.\n",
            "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# Transformer Models - Small Demos\n",
        "# -------------------------------------------------------------\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# ---- BERT Demo ----\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "inputs = bert_tokenizer(\"Transformers are amazing for NLP tasks.\", return_tensors=\"pt\")\n",
        "outputs = bert_model(**inputs)\n",
        "print(\"BERT logits:\", outputs.logits)\n",
        "\n",
        "# ---- GPT2 Demo ----\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "input_text = \"Artificial intelligence is\"\n",
        "inputs = gpt_tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = gpt_model.generate(**inputs, max_length=50)\n",
        "print(\"GPT2 text:\", gpt_tokenizer.decode(outputs[0]))\n",
        "\n",
        "# ---- T5 Demo ----\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "input_text = \"summarize: Transformers are powerful models for NLP.\"\n",
        "inputs = t5_tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = t5_model.generate(**inputs, max_length=50)\n",
        "print(\"T5 summary:\", t5_tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxoTmyTigm6-",
        "outputId": "040a0371-5863-4e26-d7db-a5dc5edd31ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT logits: tensor([[-0.1865, -0.0302]], grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2 text: Artificial intelligence is a new field of research that has been in the works for a while now. It is a field that has been in the works for a while now. It is a field that has been in the works for a while now.\n",
            "T5 summary: <pad> Transformers are powerful models for NLP.</s>\n"
          ]
        }
      ]
    }
  ]
}
